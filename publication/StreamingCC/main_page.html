<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" href="../../style.css">
</head>
<body>

<div id="content">

<h2  id = "research" style="font-weight:bold;font-family:times;text-align:center;padding-bottom:30px;"> 
	Streaming Algorithms and Lower Bounds for Estimating Correlation Clustering Cost </h2> 
<div class="wrapper">
	<div style="padding-bottom:10px">
	<span style="font-weight:bold"> Authors: </span>
	<span id="pub" style="font-size:16px">
 <span class="paper-author"> Sepehr Assadi, Vihan Shah, Chen Wang. </span>	
 </span>
	</div>
	
	<div style="padding-bottom:30px;">
	<span style="font-weight:bold;"> Conference: </span>
	<span id="pub" style="font-size:16px">
	<span class="paper-author"> 
	<a href="https://nips.cc/" class="class0">Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS'23)</a>
	</span>
	</span>
	</div>
	
	<div style="padding-bottom:50px;">
	<span style="font-weight:bold;"> Abstract: </span>
	<span id="pub" style="font-size:16px">
	<span class="paper-author"> 
		Correlation clustering is a fundamental optimization problem at the intersection of machine learning and theoretical computer science. 
		Motivated by applications to big data processing, recent years have witnessed a flurry of results on this problem in the streaming model. 
		In this model, the algorithm needs to process the input n-vertex graph by making one or few passes over the stream of its edges and using a limited 
		memory, much smaller than the input size. 
		<br/><br/>
				
		All previous work on streaming correlation clustering have focused on semi-streaming algorithms with Î©(n) memory, whereas in this work, 
		we study streaming algorithms with much smaller memory requirement of only polylog(n) bits. This stringent memory requirement is in the 
		same spirit of classical streaming algorithms that instead of recovering a full solution to the problem---which can be prohibitively large
		with such small memory as is the case in our problem---, aimed to learn certain statistical properties of their inputs. In our case, 
		this translates to determining the ``(correlation) clusterability'' of input graphs, or more precisely, estimating the cost of the optimal correlation clustering solution. 
		<br/><br/>

		As our main result, we present two novel algorithms that in only polylog(n) space are able to estimate the optimal correlation clustering cost up to 
		some constant multiplicative factor plus some extra additive error. One of the algorithms outputs a 3-multiplicative approximation plus o(n^2) additive 
		approximation, and the other one improves the additive error further down at the cost of increasing the multiplicative factor to some large constant. 
		We then present new lower bounds that justify these mix of both multiplicative and additive error approximation in our algorithms. 

</span>
</span>

</div>



<div style=padding-bottom:10px;>
	<span style="font-weight:bold;"> Conference version: </span>
	<span id="pub" style="font-size:16px">
	<span class="paper-author"> 
		<a href="https://neurips.cc/virtual/2023/poster/70662" class="class0">[Website]</a> (This also includes a short talk and its slides)
</span>
</span>
</div>



<div class="spacer"></div>
</div>

<!--END of content-->
</div>	

</body>
</html>